15-Day Python Learning Plan: Advanced and Specialized Topics

Week 1: Data Science and Visualization
Goal: Learn tools and techniques for data manipulation, analysis, and visualization.

Day 1: NumPy Basics

Topics: Arrays, array operations, indexing, slicing, broadcasting, mathematical functions.
Why: NumPy is foundational for numerical computing and data science.
Task: Create a NumPy array, perform matrix operations, and compute statistical metrics (mean, median, std).
Resources: NumPy documentation, w3schools NumPy tutorial.

Day 2: Pandas Basics

Topics: Series, DataFrames, indexing, filtering, grouping, merging.
Why: Pandas is essential for data manipulation and analysis.
Task: Load a CSV dataset (e.g., from Kaggle), filter rows, and compute summary statistics.
Resources: Pandas documentation, Real Python Pandas guide.

Day 3: Data Visualization with Matplotlib

Topics: Line plots, scatter plots, histograms, customizing plots (labels, legends, styles).
Why: Visualization is key for data exploration and communication.
Task: Create a line plot and histogram using a dataset from Day 2.
Resources: Matplotlib tutorials, Seaborn for enhanced visuals.

Day 4: Advanced Visualization with Seaborn

Topics: Heatmaps, pair plots, box plots, violin plots, styling.
Why: Seaborn simplifies complex visualizations and integrates with Pandas.
Task: Generate a heatmap and pair plot for a dataset’s correlations.
Resources: Seaborn documentation, DataCamp Seaborn tutorial.

Day 5: Data Cleaning and Preprocessing

Topics: Handling missing data, encoding categorical variables, scaling features.
Why: Real-world data is messy; preprocessing is critical for analysis.
Task: Clean a dataset by handling missing values and encoding categorical columns using Pandas.
Resources: Towards Data Science articles, Pandas preprocessing guides.

Day 6: Introduction to SQL with Python

Topics: SQLite, connecting to databases, basic queries (SELECT, INSERT, JOIN).
Why: SQL is widely used for data storage and retrieval.
Task: Create a SQLite database, insert data, and query it using sqlite3.
Resources: Real Python SQLite tutorial, w3schools SQL basics.

Day 7: Mini Project: Data Analysis Pipeline

Topics: Combine NumPy, Pandas, Matplotlib/Seaborn, and SQLite.
Why: Apply skills to a real-world problem.
Task: Build a pipeline to load, clean, analyze, and visualize a dataset (e.g., sales or weather data).
Resources: Kaggle datasets, YouTube data analysis tutorials.


Week 2: Web Development and Automation
Goal: Explore web development, APIs, and automation to broaden your Python skills.

Day 8: Web Scraping with BeautifulSoup

Topics: HTML basics, scraping with requests and BeautifulSoup, parsing tags.
Why: Web scraping is useful for collecting data from websites.
Task: Scrape a website (e.g., quotes or product prices) and save data to a CSV.
Resources: BeautifulSoup documentation, Automate the Boring Stuff (web scraping chapter).

Day 9: APIs and requests Library

Topics: REST APIs, GET/POST requests, JSON handling, authentication.
Why: APIs are a common way to fetch data programmatically.
Task: Query a public API (e.g., OpenWeatherMap or GitHub API) and process the JSON response.
Resources: Real Python requests guide, public APIs list (e.g., RapidAPI).

Day 10: Web Development with Flask

Topics: Flask setup, routes, templates, handling forms.
Why: Flask is a lightweight framework for building web apps.
Task: Create a simple Flask app with a homepage and a form to collect user input.
Resources: Flask quickstart, Corey Schafer Flask tutorials.

Day 11: Automation with pyautogui or selenium

Topics: Browser automation (Selenium), GUI automation (PyAutoGUI).
Why: Automation saves time on repetitive tasks.
Task: Automate a task, like filling a web form with Selenium or simulating mouse clicks with PyAutoGUI.
Resources: Automate the Boring Stuff, Selenium documentation.

Day 12: Asynchronous Programming with asyncio

Topics: Async/await, event loops, coroutines, aiohttp for async requests.
Why: Async programming improves performance for I/O-bound tasks.
Task: Write an async script to fetch data from multiple APIs concurrently using aiohttp.
Resources: Real Python asyncio guide, Python async documentation.


Week 3: Specialized Topics and Capstone
Goal: Dive into advanced tools and build a comprehensive project.

Day 13: Introduction to Machine Learning with Scikit-learn

Topics: Supervised learning (regression, classification), train-test split, basic models (e.g., linear regression, decision trees).
Why: Machine learning is a high-demand skill.
Task: Build a simple model to predict a target variable (e.g., house prices) using a Kaggle dataset.
Resources: Scikit-learn tutorials, Kaggle ML courses.

Day 14: Working with Databases (NoSQL with MongoDB)

Topics: MongoDB basics, pymongo, CRUD operations.
Why: NoSQL databases are popular for unstructured data.
Task: Store and query data in a MongoDB database using pymongo.
Resources: MongoDB University, Real Python MongoDB guide.

Day 15: Capstone Project and Review

Topics: Combine multiple skills (e.g., web scraping, APIs, data analysis, Flask, or ML).
Why: Synthesize knowledge into a portfolio-worthy project.
Task: Build a project, such as a web app that scrapes data, stores it in MongoDB, analyzes it with Pandas, and displays results with Flask or visualizations.
Resources: GitHub for project hosting, Kaggle for inspiration.


Tips for Success

Practice Daily: Spend 1–2 hours coding and experimenting with each topic.
Use GitHub: Push your code to GitHub for version control and portfolio building.
Resources: Leverage free resources like Real Python, Towards Data Science, and YouTube tutorials (e.g., Corey Schafer, Tech With Tim).
Debugging: Use tools like pdb or IDE debuggers to troubleshoot.
Community: Engage on X or Stack Overflow for help and inspiration.

This plan builds on your existing Python skills, introducing practical tools for data science, web development, and automation. If you want a deeper dive into any topic or specific resources, let me know!

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 1: NumPy Basics
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Day 1: NumPy Basics - Complete Guide

What is NumPy?
NumPy (Numerical Python) is the foundational package for scientific computing in Python. It provides:

Multidimensional arrays (ndarrays) with fast mathematical operations
Broadcasting for operations on arrays of different shapes
Efficient memory usage and C-level speed
Linear algebra, Fourier transforms, and random number capabilities

Installation: pip install numpy
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1. Creating NumPy Arrays
Basic Array Creation
pythonimport numpy as np

# From Python lists
arr1 = np.array([1, 2, 3, 4, 5])
print(arr1)  # [1 2 3 4 5]
print(type(arr1))  # <class 'numpy.ndarray'>

# 2D array (matrix)
arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print(arr2d)
# [[1 2 3]
#  [4 5 6]
#  [7 8 9]]

# Array of zeros
zeros = np.zeros((3, 4))  # 3 rows, 4 columns
print(zeros)
# [[0. 0. 0. 0.]
#  [0. 0. 0. 0.]
#  [0. 0. 0. 0.]]

# Array of ones
ones = np.ones((2, 3))
print(ones)
# [[1. 1. 1.]
#  [1. 1. 1.]]

# Array with specific value
full = np.full((2, 2), 7)
print(full)
# [[7 7]
#  [7 7]]
Array Properties
pythonarr = np.array([[1, 2, 3], [4, 5, 6]])

print("Shape:", arr.shape)        # (2, 3) - rows, columns
print("Dimensions:", arr.ndim)     # 2
print("Data type:", arr.dtype)    # int64
print("Size:", arr.size)          # 6 (total elements)
print("Item size:", arr.itemsize) # 8 bytes
Range and Random Arrays
python# Like Python range
arr_range = np.arange(0, 10, 2)  # [0 2 4 6 8]
print(arr_range)

# Evenly spaced numbers
arr_linspace = np.linspace(0, 1, 5)  # [0.   0.25 0.5  0.75 1.  ]
print(arr_linspace)

# Random numbers
np.random.seed(42)  # For reproducible results
random_arr = np.random.rand(3, 3)  # 3x3 array of random floats [0,1)
print(random_arr)

randint_arr = np.random.randint(1, 100, size=(2, 3))  # Random integers
print(randint_arr)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. Array Indexing and Slicing
1D Array Indexing
pythonarr = np.array([10, 20, 30, 40, 50])

print(arr[0])     # 10 (first element)
print(arr[-1])    # 50 (last element)
print(arr[1:4])   # [20 30 40]
print(arr[:3])    # [10 20 30]
print(arr[2:])    # [30 40 50]
print(arr[::2])   # [10 30 50] (every 2nd element)
2D Array Indexing
pythonarr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# Single element: arr2d[row][col] or arr2d[row, col]
print(arr2d[1, 1])  # 5 (middle element)

# Row slicing
print(arr2d[0])     # [1 2 3] (first row)
print(arr2d[0:2])   # First two rows
# [[1 2 3]
#  [4 5 6]]

# Column slicing
print(arr2d[:, 1])  # [2 5 8] (second column)

# Submatrix
print(arr2d[0:2, 1:3])
# [[2 3]
#  [5 6]]

# Boolean indexing
arr = np.array([1, 2, 3, 4, 5])
mask = arr > 3
print(mask)  # [False False False True True]
print(arr[mask])  # [4 5]

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. Array Operations
Element-wise Operations
pythonarr1 = np.array([1, 2, 3])
arr2 = np.array([4, 5, 6])

print(arr1 + arr2)  # [5 7 9]
print(arr1 * 2)     # [2 4 6]
print(arr1 ** 2)    # [1 4 9]
print(np.sqrt(arr1)) # [1. 1.414... 1.732...]Traceback (most recent call last):
  File "<string>", line 1, in <module>
NameError: name 'np' is not defined

Mathematical Functions
pythonarr = np.array([1, 4, 9, 16])

print(np.exp(arr))        # e^x for each element
print(np.log(arr))        # Natural log
print(np.sin(arr))        # Sine
print(np.cos(arr))        # Cosine
print(np.tanh(arr))       # Hyperbolic tangent

# Statistical functions
data = np.random.randn(1000)
print(np.mean(data))      # Average
print(np.median(data))    # Median
print(np.std(data))       # Standard deviation
print(np.var(data))       # Variance
print(np.min(data), np.max(data))  # Min, Max

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

4. Broadcasting
Broadcasting allows operations between arrays of different shapes by automatically expanding dimensions.
python# Scalar with array
arr = np.array([[1, 2, 3], [4, 5, 6]])
scalar = 10
print(arr + scalar)
# [[11 12 13]
#  [14 15 16]]

# Different shaped arrays
arr1 = np.array([1, 2, 3])      # Shape (3,)
arr2 = np.array([[10], [20]])   # Shape (2, 1)
result = arr1 + arr2
print(result)
# [[11 12 13]
#  [21 22 23]]

# Broadcasting rules: dimensions must be 1 or match
# The smaller dimension gets "stretched" to match the larger one

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

5. Matrix Operations
python# Matrix multiplication
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# Element-wise multiplication
print(A * B)
# [[ 5 12]
#  [21 32]]

# Matrix multiplication (dot product)
print(np.dot(A, B))
print(A @ B)  # Python 3.5+ matrix multiplication operator
# [[19 22]
#  [43 50]]

# Transpose
print(A.T)
# [[1 3]
#  [2 4]]

# Inverse (for square matrices)
from numpy.linalg import inv
print(inv(A))
# Note: Only works for square, non-singular matrices

# Determinant
print(np.linalg.det(A))

# Eigenvalues
eigenvals = np.linalg.eigvals(A)
print(eigenvals)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

6. Reshaping and Stacking
pythonarr = np.arange(12)  # [0 1 2 3 4 5 6 7 8 9 10 11]

# Reshape
reshaped = arr.reshape(3, 4)
print(reshaped)
# [[ 0  1  2  3]
#  [ 4  5  6  7]
#  [ 8  9 10 11]]

# Transpose and reshape back
print(reshaped.T.reshape(-1))  # -1 infers the size

# Stacking arrays
a1 = np.array([1, 2])
a2 = np.array([3, 4])

# Vertical stack
vstack = np.vstack((a1, a2))
print(vstack)
# [[1 2]
#  [3 4]]

# Horizontal stack
hstack = np.hstack((a1, a2))
print(hstack)  # [1 2 3 4]

# Concatenate with axis
arr1 = np.array([[1, 2], [3, 4]])
arr2 = np.array([[5, 6]])
concat = np.concatenate([arr1, arr2], axis=0)
print(concat)
# [[1 2]
#  [3 4]
#  [5 6]]

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

7. Statistical Functions
pythondata = np.random.randn(5, 3)
print("Data:\n", data)

# Axis-wise operations
print("Row means:", np.mean(data, axis=1))  # Mean of each row
print("Column means:", np.mean(data, axis=0))  # Mean of each column

# Summary statistics
print("Overall mean:", np.mean(data))
print("Standard deviation:", np.std(data))
print("Min:", np.min(data))
print("Max:", np.max(data))
print("Sum:", np.sum(data))

# Percentiles
print("25th percentile:", np.percentile(data, 25))
print("75th percentile:", np.percentile(data, 75))

# Correlation matrix
corr_matrix = np.corrcoef(data)
print("Correlation matrix:\n", corr_matrix)
Day 1 Task: Complete Implementation
pythonimport numpy as np

# Task 1: Create and explore arrays
print("=== Task 1: Array Creation ===")
# Create a 3x4 array of random integers between 1-100
np.random.seed(42)
random_matrix = np.random.randint(1, 101, size=(3, 4))
print("Random Matrix:\n", random_matrix)
print("Shape:", random_matrix.shape)
print("Data type:", random_matrix.dtype)

# Task 2: Matrix operations
print("\n=== Task 2: Matrix Operations ===")
A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
B = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]])

print("A @ B (Matrix multiplication):\n", A @ B)
print("A * B (Element-wise):\n", A * B)
print("Transpose of A:\n", A.T)

# Task 3: Statistical metrics
print("\n=== Task 3: Statistical Analysis ===")
data = np.random.randn(100)
print(f"Mean: {np.mean(data):.2f}")
print(f"Median: {np.median(data):.2f}")
print(f"Standard Deviation: {np.std(data):.2f}")
print(f"Variance: {np.var(data):.2f}")
print(f"Min: {np.min(data):.2f}, Max: {np.max(data):.2f}")

# Task 4: Advanced operations
print("\n=== Task 4: Advanced Operations ===")
# Normalize data (scale to [0,1])
normalized = (data - np.min(data)) / (np.max(data) - np.min(data))
print(f"Normalized data range: [{np.min(normalized):.2f}, {np.max(normalized):.2f}]")

# Find indices of values above mean
above_mean = np.where(data > np.mean(data))
print(f"Number of values above mean: {len(above_mean[0])}")

# Create filtered array
filtered = data[data > np.mean(data)]
print(f"Filtered data shape: {filtered.shape}")

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Takeaways

Arrays are faster than Python lists for numerical operations
Broadcasting enables flexible operations between different shaped arrays
Vectorized operations replace Python loops for better performance
Indexing works similarly to lists but supports advanced features like boolean indexing
Matrix operations are built-in and optimized
Statistical functions work on entire arrays or along specific axes

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Practice Exercises

Create a 5x5 magic square using NumPy
Implement matrix inversion for a 3x3 matrix
Generate and analyze a dataset of 1000 random numbers
Practice broadcasting by adding a vector to each row of a matrix
Use boolean indexing to filter and modify array elements

Run the task code above and experiment with different array shapes and operations. Tomorrow you'll build on this foundation with Pandas for data manipulation!

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 2: Pandas Basics
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is Pandas?
Pandas is a powerful data manipulation and analysis library built on NumPy. It provides:

Series: 1D labeled arrays
DataFrames: 2D labeled data tables (like Excel or SQL tables)
Time series functionality
Data cleaning and transformation tools
Integration with NumPy, Matplotlib, and other libraries

Installation: pip install pandas
pythonimport pandas as pd
import numpy as np
print(pd.__version__)  # Check pandas version

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1. Pandas Series

Creating Series
python# From Python list
s1 = pd.Series([1, 2, 3, 4, 5])
print(s1)
# 0    1
# 1    2
# 2    3
# 3    4
# 4    5
# dtype: int64

# With custom index
s2 = pd.Series([10, 20, 30, 40], index=['a', 'b', 'c', 'd'])
print(s2)
# a    10
# b    20
# c    30
# d    40
# dtype: int64

# From dictionary
s3 = pd.Series({'name': 'Alice', 'age': 25, 'city': 'NYC'})
print(s3)
# name    Alice
# age        25
# city      NYC
# dtype: object

# From NumPy array
arr = np.random.randn(5)
s4 = pd.Series(arr, name='random_values')
print(s4)
Series Operations
pythons = pd.Series([1, 2, 3, 4, 5], index=['a', 'b', 'c', 'd', 'e'])

# Indexing
print(s['a'])      # 1
print(s[0])        # 1 (integer position)
print(s['a':'c'])  # a    1
                   # b    2
                   # c    3

# Mathematical operations
print(s * 2)       # Multiply all values by 2
print(s > 2)       # Boolean mask
print(s[s > 2])    # Filter values > 2

# Statistical methods
print(s.mean())    # 3.0
print(s.describe()) # Summary statistics

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. DataFrames

Creating DataFrames
python# From dictionary
data = {
    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'age': [25, 30, 35, 28],
    'city': ['NYC', 'LA', 'Chicago', 'Boston'],
    'salary': [70000, 80000, 90000, 65000]
}
df = pd.DataFrame(data)
print(df)
#     name  age     city  salary
# 0  Alice   25      NYC   70000
# 1    Bob   30       LA   80000
# 2  Charlie  35  Chicago   90000
# 3  Diana   28   Boston   65000

# From NumPy array
arr = np.random.randn(3, 4)
df2 = pd.DataFrame(arr, columns=['A', 'B', 'C', 'D'], index=['row1', 'row2', 'row3'])
print(df2)

# Empty DataFrame with custom index/columns
df_empty = pd.DataFrame(index=['a', 'b', 'c'], columns=['x', 'y'])
print(df_empty)
DataFrame Attributes and Inspection
python# Basic info
print("Shape:", df.shape)          # (4, 4)
print("Columns:", df.columns)      # Index(['name', 'age', 'city', 'salary'], ...)
print("Index:", df.index)          # RangeIndex(start=0, stop=4, step=1)
print(df.info())                   # Data types and memory usage
print(df.dtypes)                   # Column data types
print(df.head(2))                  # First 2 rows
print(df.tail())                   # Last rows
print(df.describe())               # Numerical summary statistics

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. Indexing and Selecting Data

Column Selection
python# Single column (returns Series)
ages = df['age']
print(ages)
print(type(ages))  # <class 'pandas.core.series.Series'>

# Multiple columns (returns DataFrame)
subset = df[['name', 'salary']]
print(subset)
print(type(subset))  # <class 'pandas.core.frame.DataFrame'>

# Column access with dot notation (if no spaces/special chars)
print(df.age)  # Same as df['age']
Row Selection
python# By integer position - iloc
print(df.iloc[0])      # First row
print(df.iloc[0:2])    # First two rows
print(df.iloc[-1])     # Last row
print(df.iloc[:, 1])   # Second column (age)

# By label - loc
print(df.loc[0])       # Row with index 0
print(df.loc[0, 'name'])  # Specific cell
print(df.loc[0:2, ['name', 'city']])  # Rows 0-2, specific columns

# Boolean indexing
high_salary = df['salary'] > 70000
print(df[high_salary])  # Rows where salary > 70000

# Multiple conditions
condition = (df['age'] > 25) & (df['city'] == 'NYC')
print(df[condition])
Setting Values
python# Add new column
df['department'] = ['IT', 'HR', 'Finance', 'Marketing']

# Modify existing column
df['salary'] = df['salary'] * 1.1  # 10% raise

# Set value by condition
df.loc[df['age'] > 30, 'senior'] = 'Yes'
df.loc[df['age'] <= 30, 'senior'] = 'No'

# Drop columns/rows
df_dropped = df.drop('department', axis=1)  # axis=1 for columns
df_no_first = df.drop(0, axis=0)           # axis=0 for rows

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

4. Data Loading and CSV Operations

Reading CSV Files
python# Create sample CSV data
sample_data = {
    'product': ['Laptop', 'Phone', 'Tablet', 'Monitor'],
    'price': [999, 599, 299, 199],
    'quantity': [10, 25, 50, 15],
    'category': ['Electronics', 'Electronics', 'Electronics', 'Electronics']
}

# Save to CSV
df_sample = pd.DataFrame(sample_data)
df_sample.to_csv('products.csv', index=False)

# Read CSV
df_products = pd.read_csv('products.csv')
print(df_products)

# Read with options
df_custom = pd.read_csv('products.csv', 
                       index_col='product',  # Set product as index
                       usecols=['product', 'price', 'quantity'])  # Select specific columns
print(df_custom)
Other Data Formats
python# Excel files
# df.to_excel('data.xlsx', index=False)
# df = pd.read_excel('data.xlsx')

# JSON
# df.to_json('data.json', orient='records')
# df = pd.read_json('data.json')

# Pickle (fast serialization)
# df.to_pickle('data.pkl')
# df = pd.read_pickle('data.pkl')

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

5. Data Cleaning and Manipulation

Handling Missing Data
python# Create DataFrame with missing values
df_missing = pd.DataFrame({
    'A': [1, 2, np.nan, 4],
    'B': [5, np.nan, 7, 8],
    'C': [9, 10, 11, 12]
})

print("Original:\n", df_missing)
print("Missing values:\n", df_missing.isnull())

# Drop missing values
df_dropped = df_missing.dropna()  # Drop any row with NaN
print("Drop NaN:\n", df_dropped)

# Fill missing values
df_filled = df_missing.fillna(0)  # Fill with 0
print("Fill with 0:\n", df_filled)

df_forward = df_missing.fillna(method='ffill')  # Forward fill
print("Forward fill:\n", df_forward)

# Fill with mean
df_mean = df_missing.copy()
df_mean['A'] = df_mean['A'].fillna(df_mean['A'].mean())
print("Fill with mean:\n", df_mean)
Data Type Conversion
python# Convert data types
df['age'] = df['age'].astype('float64')  # To float
df['salary'] = pd.to_numeric(df['salary'])  # Ensure numeric

# Convert to categorical
df['city'] = df['city'].astype('category')
print(df['city'].cat.categories)

# DateTime conversion
dates = pd.Series(['2023-01-01', '2023-02-01', '2023-03-01'])
dates = pd.to_datetime(dates)
print(dates)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

6. Grouping and Aggregation

GroupBy Operations
python# Sample data for grouping
sales_data = pd.DataFrame({
    'region': ['North', 'South', 'North', 'South', 'East', 'West'],
    'sales': [100, 150, 200, 120, 180, 90],
    'product': ['A', 'B', 'A', 'B', 'A', 'B']
})

# Group by single column
grouped = sales_data.groupby('region')
print(grouped['sales'].mean())  # Average sales per region

# Multiple aggregations
result = sales_data.groupby('region').agg({
    'sales': ['sum', 'mean', 'count'],
    'product': 'count'
})
print(result)

# Group by multiple columns
multi_group = sales_data.groupby(['region', 'product'])['sales'].sum()
print(multi_group)

# Apply custom function
def sales_category(x):
    if x.mean() > 120:
        return 'High'
    else:
        return 'Low'

sales_data['category'] = sales_data.groupby('region')['sales'].transform(sales_category)
print(sales_data)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

7. Merging and Joining DataFrames

Concatenation
pythondf1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})

# Vertical concatenation
concat_v = pd.concat([df1, df2], axis=0, ignore_index=True)
print("Vertical concat:\n", concat_v)

# Horizontal concatenation
concat_h = pd.concat([df1, df2], axis=1)
print("Horizontal concat:\n", concat_h)
Merging (SQL-like joins)
python# Sample data for merging
employees = pd.DataFrame({
    'emp_id': [1, 2, 3, 4],
    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],
    'dept_id': [10, 20, 10, 30]
})

departments = pd.DataFrame({
    'dept_id': [10, 20, 30],
    'dept_name': ['IT', 'HR', 'Finance']
})

# Inner join (default)
merged = pd.merge(employees, departments, on='dept_id', how='inner')
print("Inner join:\n", merged)

# Left join
left_join = pd.merge(employees, departments, on='dept_id', how='left')
print("Left join:\n", left_join)

# Right join
right_join = pd.merge(employees, departments, on='dept_id', how='right')

# Outer join
outer_join = pd.merge(employees, departments, on='dept_id', how='outer')

# Multiple keys
# pd.merge(df1, df2, on=['key1', 'key2'])

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Day 2 Task: Complete Implementation

pythonimport pandas as pd
import numpy as np

print("=== Pandas Data Analysis Task ===")

# Step 1: Create and load sample dataset
print("\n1. Creating and loading sample sales data...")

# Generate sample sales data
np.random.seed(42)
n_rows = 100
sales_data = {
    'date': pd.date_range('2023-01-01', periods=n_rows, freq='D'),
    'product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor'], n_rows),
    'region': np.random.choice(['North', 'South', 'East', 'West'], n_rows),
    'quantity': np.random.randint(1, 20, n_rows),
    'price': np.random.uniform(100, 1000, n_rows),
    'salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], n_rows)
}

df = pd.DataFrame(sales_data)
df['total_sales'] = df['quantity'] * df['price']

# Save to CSV
df.to_csv('sales_data.csv', index=False)
print(f"Dataset shape: {df.shape}")
print("\nFirst few rows:")
print(df.head())

# Step 2: Data exploration and summary statistics
print("\n2. Summary Statistics:")
print(df.describe())
print("\nData types:")
print(df.dtypes)
print("\nMissing values:")
print(df.isnull().sum())

# Step 3: Filtering rows
print("\n3. Filtering Operations:")

# Filter high-value sales
high_value = df[df['total_sales'] > df['total_sales'].quantile(0.75)]
print(f"High-value sales (top 25%): {len(high_value)} rows")
print(high_value[['product', 'region', 'total_sales']].head())

# Filter by region and product
laptop_north = df[(df['product'] == 'Laptop') & (df['region'] == 'North')]
print(f"Laptop sales in North: {len(laptop_north)}")
print(laptop_north['total_sales'].sum())

# Step 4: Grouping and aggregation
print("\n4. Grouping Analysis:")

# Sales by region
region_sales = df.groupby('region')['total_sales'].agg(['sum', 'mean', 'count'])
print("Sales by region:")
print(region_sales)

# Sales by product
product_analysis = df.groupby('product').agg({
    'total_sales': ['sum', 'mean'],
    'quantity': 'sum',
    'price': 'mean'
})
print("\nProduct analysis:")
print(product_analysis)

# Top performing salesperson
salesperson_rank = df.groupby('salesperson')['total_sales'].sum().sort_values(ascending=False)
print("\nTop salesperson:")
print(salesperson_rank)

# Step 5: Advanced filtering and merging
print("\n5. Advanced Analysis:")

# Create customer segment based on total sales
def segment_sales(total):
    if total > 5000:
        return 'High'
    elif total > 2000:
        return 'Medium'
    else:
        return 'Low'

df['sales_segment'] = df['total_sales'].apply(segment_sales)

# Analyze segments
segment_analysis = df.groupby('sales_segment')['total_sales'].agg(['count', 'sum', 'mean'])
print("Sales by segment:")
print(segment_analysis)

# Create a summary DataFrame for regions
region_summary = df.groupby('region').agg({
    'total_sales': 'sum',
    'quantity': 'sum',
    'price': 'mean'
}).round(2)

print("\nRegion summary:")
print(region_summary)

# Step 6: Export results
region_summary.to_csv('region_summary.csv')
print("\nRegion summary exported to 'region_summary.csv'")

# Step 7: Advanced filtering example
print("\n6. Complex Filtering:")
# Find top 5 sales in each region
top_sales_per_region = df.loc[df.groupby('region')['total_sales'].nlargest(5).index]
print(f"Top 5 sales per region: {len(top_sales_per_region)} records")
print(top_sales_per_region[['region', 'product', 'total_sales']].head(10))

print("\n=== Task Complete! ===")
print(f"Dataset analyzed: {len(df)} records")
print(f"Total sales: ${df['total_sales'].sum():,.2f}")
print(f"Average sale: ${df['total_sales'].mean():.2f}")

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Takeaways

Series are 1D labeled arrays, DataFrames are 2D labeled tables
Indexing: Use loc (label-based), iloc (position-based), and boolean indexing
GroupBy enables split-apply-combine operations for aggregation
Merging works like SQL JOINs with different strategies (inner, left, outer)
Missing data handling: drop, fill, or interpolate
CSV operations are seamless with read_csv() and to_csv()
Chaining operations improves readability and performance

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Practice Exercises

Load a real dataset from Kaggle and explore its structure
Create pivot tables for multi-dimensional analysis
Implement data validation and cleaning pipeline
Practice complex merging scenarios with multiple keys
Use apply(), map(), and transform() for custom operations

Run the task code to analyze the generated sales dataset. Tomorrow you'll visualize this data with Matplotlib and Seaborn!

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 3: Data Visualization with Matplotlib
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is Matplotlib?

Matplotlib is a versatile plotting library for Python that enables the creation of static, animated, and interactive visualizations. It is widely used for data exploration and presentation, integrating seamlessly with NumPy and Pandas.

Installation: pip install matplotlib

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1. Getting Started with Matplotlib

Matplotlib's primary module is pyplot, which provides a MATLAB-like interface for plotting.
pythonimport matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Basic setup
plt.figure()  # Create a new figure
plt.plot([1, 2, 3, 4], [10, 20, 25, 30])  # Simple line plot
plt.title("Basic Line Plot")
plt.xlabel("X-axis")
plt.ylabel("Y-axis")
plt.show()  # Display the plot

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. Line Plots
Line plots are used to visualize trends or relationships between two variables.
python# Sample line plot
x = np.linspace(0, 10, 100)  # 100 points from 0 to 10
y1 = np.sin(x)
y2 = np.cos(x)

plt.figure(figsize=(8, 5))  # Set figure size
plt.plot(x, y1, label='sin(x)', color='blue', linestyle='--', linewidth=2)
plt.plot(x, y2, label='cos(x)', color='red', linestyle='-', linewidth=2)
plt.title('Sine and Cosine Functions')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()  # Show legend
plt.grid(True)  # Add grid
plt.show()
Customizing Line Plots

Colors: 'blue', 'red', '#FF5733', or RGB tuples (0.1, 0.2, 0.3)
Linestyles: '-' (solid), '--' (dashed), ':' (dotted), '-.' (dash-dot)
Markers: 'o' (circle), 's' (square), '^' (triangle), '*' (star)
Linewidth: Controls thickness (e.g., linewidth=2)

pythonplt.plot(x, y1, 'b--o', label='sin(x)')  # Blue, dashed, circle markers
plt.plot(x, y2, 'r-s', label='cos(x)')   # Red, solid, square markers
plt.legend()
plt.show()

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. Scatter Plots
Scatter plots visualize individual data points to identify patterns or clusters.
python# Sample scatter plot
np.random.seed(42)
x = np.random.randn(100)
y = np.random.randn(100)

plt.figure(figsize=(6, 4))
plt.scatter(x, y, color='green', alpha=0.5, s=100, label='Random Points')
plt.title('Scatter Plot of Random Points')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.grid(True)
plt.show()
Customizing Scatter Plots

s: Marker size (e.g., s=100)
alpha: Transparency (0 to 1)
c: Color or array for color mapping
marker: Shape of markers ('o', 's', etc.)

python# Scatter with varying sizes and colors
sizes = np.random.randint(50, 200, 100)
colors = np.random.rand(100)
plt.scatter(x, y, s=sizes, c=colors, cmap='viridis', alpha=0.7)
plt.colorbar(label='Color Intensity')
plt.title('Customized Scatter Plot')
plt.show()

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

4. Histograms
Histograms display the distribution of a single variable.
python# Sample histogram
data = np.random.randn(1000)

plt.figure(figsize=(6, 4))
plt.hist(data, bins=30, color='purple', alpha=0.7, edgecolor='black')
plt.title('Histogram of Random Normal Data')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)
plt.show()
Customizing Histograms

bins: Number of bins or specific bin edges (e.g., bins=30 or bins=[-3, -2, -1, 0, 1, 2, 3])
alpha: Transparency
edgecolor: Outline color for bars
density: Normalize to probability density (sums to 1)

python# Multiple histograms
data1 = np.random.randn(1000)
data2 = np.random.randn(1000) + 1

plt.hist(data1, bins=30, alpha=0.5, label='Data 1', color='blue')
plt.hist(data2, bins=30, alpha=0.5, label='Data 2', color='orange')
plt.title('Overlaid Histograms')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.show()

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

5. Customizing Plots
Matplotlib allows extensive customization for professional visualizations.
Labels and Titles
pythonplt.plot(x, y1, label='sin(x)')
plt.title('Sine Function', fontsize=14, pad=10)
plt.xlabel('X-axis', fontsize=12)
plt.ylabel('Y-axis', fontsize=12)
plt.legend(fontsize=10)
plt.show()
Styles
Matplotlib offers built-in styles for consistent aesthetics.
python# List available styles
print(plt.style.available)

# Apply a style
plt.style.use('seaborn')  # Use 'seaborn' style
plt.plot(x, y1, label='sin(x)')
plt.title('Sine with Seaborn Style')
plt.legend()
plt.show()

# Reset to default
plt.style.use('default')
Subplots
Create multiple plots in a single figure.
pythonfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))  # 1 row, 2 columns

ax1.plot(x, y1, 'b-', label='sin(x)')
ax1.set_title('Sine')
ax1.set_xlabel('x')
ax1.set_ylabel('sin(x)')
ax1.legend()

ax2.plot(x, y2, 'r-', label='cos(x)')
ax2.set_title('Cosine')
ax2.set_xlabel('x')
ax2.set_ylabel('cos(x)')
ax2.legend()

plt.tight_layout()  # Adjust spacing
plt.show()
Saving Plots
pythonplt.plot(x, y1)
plt.savefig('sine_plot.png', dpi=300, bbox_inches='tight')
plt.close()  # Close figure to free memory

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Day 3 Task: Complete Implementation

Using the sales dataset from Day 2, create a line plot of total sales over time and a histogram of total sales distribution.matplotlib_sales_plots.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Step 1: Load the sales dataset from Day 2
print("=== Matplotlib Visualization Task ===")
print("\n1. Loading sales data...")

# Generate sample sales data (same as Day 2)
np.random.seed(42)
n_rows = 100
sales_data = {
    'date': pd.date_range('2023-01-01', periods=n_rows, freq='D'),
    'product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor'], n_rows),
    'region': np.random.choice(['North', 'South', 'East', 'West'], n_rows),
    'quantity': np.random.randint(1, 20, n_rows),
    'price': np.random.uniform(100, 1000, n_rows),
    'salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], n_rows)
}
df = pd.DataFrame(sales_data)
df['total_sales'] = df['quantity'] * df['price']

print(f"Dataset shape: {df.shape}")
print(df.head())

# Step 2: Line plot - Total sales over time
print("\n2. Creating line plot of total sales over time...")

# Group by date to get daily total sales
daily_sales = df.groupby('date')['total_sales'].sum()

plt.figure(figsize=(10, 6))
plt.plot(daily_sales.index, daily_sales.values, color='blue', linestyle='-', marker='o', markersize=4, label='Daily Total Sales')
plt.title('Total Sales Over Time (Jan-Apr 2023)', fontsize=14, pad=10)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Total Sales ($)', fontsize=12)
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)
plt.legend()
plt.tight_layout()
plt.savefig('daily_sales_line_plot.png', dpi=300, bbox_inches='tight')
plt.show()

# Step 3: Histogram - Distribution of total sales
print("\n3. Creating histogram of total sales distribution...")

plt.figure(figsize=(8, 5))
plt.hist(df['total_sales'], bins=20, color='green', alpha=0.7, edgecolor='black')
plt.title('Distribution of Total Sales', fontsize=14)
plt.xlabel('Total Sales ($)', fontsize=12)
plt.ylabel('Frequency', fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('sales_histogram.png', dpi=300, bbox_inches='tight')
plt.show()

# Step 4: Additional visualization - Scatter plot of quantity vs. total sales
print("\n4. Creating scatter plot of quantity vs. total sales...")

plt.figure(figsize=(8, 5))
for product in df['product'].unique():
    subset = df[df['product'] == product]
    plt.scatter(subset['quantity'], subset['total_sales'], label=product, alpha=0.6, s=100)
plt.title('Quantity vs. Total Sales by Product', fontsize=14)
plt.xlabel('Quantity Sold', fontsize=12)
plt.ylabel('Total Sales ($)', fontsize=12)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('quantity_sales_scatter.png', dpi=300, bbox_inches='tight')
plt.show()

# Step 5: Summary statistics for context
print("\n5. Summary Statistics:")
print(df['total_sales'].describe())
print(f"\nTotal Sales: ${df['total_sales'].sum():,.2f}")
print(f"Average Sale: ${df['total_sales'].mean():.2f}")

print("\n=== Task Complete! ===")
print("Plots saved as 'daily_sales_line_plot.png', 'sales_histogram.png', and 'quantity_sales_scatter.png'")

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

•Key Takeaways

Line Plots: Ideal for trends over continuous variables (e.g., time)
Scatter Plots: Show relationships or clusters in data
Histograms: Reveal data distribution
Customization: Titles, labels, legends, and grids enhance readability
Subplots: Organize multiple plots in one figure
Styles: Use built-in styles like 'seaborn' for polished visuals
Saving: Export high-quality plots with savefig()

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Practice Exercises

Create a multi-line plot comparing sales by region over time
Generate a histogram with normalized density
Build a subplot grid with different plot types
Experiment with different Matplotlib styles
Add annotations (e.g., text or arrows) to highlight key data points

Run the task code to generate the visualizations. Tomorrow, you'll enhance these plots with Seaborn for more advanced visualizations!

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 4: Advanced Visualization with Seaborn
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is Seaborn?

Seaborn is a Python visualization library built on Matplotlib, designed for easier and more attractive statistical plots. It integrates well with Pandas and simplifies complex visualizations.
Installation: pip install seaborn

import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1. Key Seaborn Plots

Heatmap

Shows data as a color-coded matrix, ideal for correlations.

# Sample data

data = np.random.rand(5, 5)
sns.heatmap(data, annot=True, cmap='coolwarm')
plt.title('Sample Heatmap')
plt.show()

Pair Plot

Displays pairwise relationships and distributions for multiple variables.
python# Sample DataFrame
df = pd.DataFrame({
    'A': np.random.randn(100),
    'B': np.random.randn(100) * 2,
    'C': np.random.randn(100) + 1
})
sns.pairplot(df)
plt.show()

Box Plot

Shows distribution, quartiles, and outliers.
pythonsns.boxplot(x=df['A'])
plt.title('Box Plot of A')
plt.show()

Violin Plot

Combines box plot with density, showing distribution shape.
pythonsns.violinplot(x=df['A'])
plt.title('Violin Plot of A')
plt.show()

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. Styling

Seaborn offers built-in themes for polished visuals.

# Set style

sns.set_style('darkgrid')  # Options: darkgrid, whitegrid, dark, white, ticks

# Apply context for scaling

sns.set_context('notebook')  # Options: paper, notebook, talk, poster

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Day 4 Task: Heatmap and Pair Plot

Using the sales dataset from Day 2, create a heatmap of correlations and a pair plot for numerical columns.

# Load sales dataset from Day 2

np.random.seed(42)
n_rows = 100
sales_data = {
    'date': pd.date_range('2023-01-01', periods=n_rows, freq='D'),
    'product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor'], n_rows),
    'region': np.random.choice(['North', 'South', 'East', 'West'], n_rows),
    'quantity': np.random.randint(1, 20, n_rows),
    'price': np.random.uniform(100, 1000, n_rows),
    'salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], n_rows)
}
df = pd.DataFrame(sales_data)
df['total_sales'] = df['quantity'] * df['price']

# Step 1: Heatmap of correlations
print("1. Creating correlation heatmap...")
corr = df[['quantity', 'price', 'total_sales']].corr()
sns.heatmap(corr, annot=True, cmap='viridis', center=0)
plt.title('Correlation Heatmap of Sales Data')
plt.savefig('correlation_heatmap.png')
plt.show()

# Step 2: Pair plot of numerical columns
print("\n2. Creating pair plot...")
sns.pairplot(df[['quantity', 'price', 'total_sales']])
plt.savefig('sales_pairplot.png')
plt.show()

print("\nTask Complete! Plots saved as 'correlation_heatmap.png' and 'sales_pairplot.png'")

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Takeaways

Heatmaps: Visualize correlations or matrices.
Pair Plots: Show relationships and distributions.
Box/Violin Plots: Display distribution and outliers.
Styling: Themes like darkgrid enhance visuals.
Pandas Integration: Works seamlessly with DataFrames.

Run the task code to generate the plots. Tomorrow, you'll tackle data cleaning and preprocessing!

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 5: Data Cleaning and Preprocessing
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is Data Cleaning and Preprocessing?

Data cleaning and preprocessing involve preparing raw data for analysis by handling missing values, encoding categorical variables, and scaling numerical features. This ensures data is consistent and suitable for modeling or visualization.
Dependencies: pandas, numpy, scikit-learn

Installation: pip install pandas numpy scikit-learn

pythonimport pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1. Handling Missing Data

Detecting Missing Values

# Sample DataFrame with missing values
df = pd.DataFrame({
    'A': [1, 2, np.nan, 4],
    'B': [5, np.nan, 7, 8],
    'C': ['x', 'y', np.nan, 'z']
})
print("Missing values:\n", df.isnull())

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Handling Options

Drop: Remove rows/columns with missing values.
Fill: Replace missing values with a constant, mean, or other strategy.

# Drop rows with any missing values
df_dropped = df.dropna()
print("Dropped:\n", df_dropped)

# Fill missing values with 0
df_filled = df.fillna(0)
print("Filled with 0:\n", df_filled)

# Fill numerical column with mean
df['A'] = df['A'].fillna(df['A'].mean())
print("Filled A with mean:\n", df)

# Fill categorical column with mode
df['C'] = df['C'].fillna(df['C'].mode()[0])
print("Filled C with mode:\n", df)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. Encoding Categorical Variables

Categorical data must be converted to numerical formats for analysis or modeling.

Label Encoding
Assigns integers to categories (useful for ordinal data).

# Label encoding
df['C_encoded'] = df['C'].astype('category').cat.codes
print("Label encoded:\n", df)

One-Hot Encoding
Creates binary columns for each category (for nominal data).

# One-hot encoding
df_onehot = pd.get_dummies(df, columns=['C'])
print("One-hot encoded:\n", df_onehot)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. Scaling Features

Scaling ensures numerical features are on the same scale, critical for many algorithms.

Standardization (Z-score)
Scales data to mean=0, std=1.

pythonscaler = StandardScaler()
df[['A', 'B']] = scaler.fit_transform(df[['A', 'B']])
print("Standardized:\n", df)

Min-Max Scaling
Scales data to a range (e.g., [0, 1]).

pythonminmax_scaler = MinMaxScaler()
df[['A', 'B']] = minmax_scaler.fit_transform(df[['A', 'B']])
print("Min-Max scaled:\n", df)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Day 5 Task: Clean and Preprocess Sales Dataset

Using the sales dataset from Day 2, handle missing values, encode categorical columns, and scale numerical features.preprocess_sales.pypython

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Step 1: Load and create sample sales dataset
print("=== Data Cleaning and Preprocessing Task ===")
print("\n1. Loading sales data...")

np.random.seed(42)
n_rows = 100
sales_data = {
    'product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor'], n_rows),
    'region': np.random.choice(['North', 'South', 'East', 'West'], n_rows),
    'quantity': np.random.randint(1, 20, n_rows),
    'price': np.random.uniform(100, 1000, n_rows),
    'salesperson': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], n_rows)
}
df = pd.DataFrame(sales_data)
df['total_sales'] = df['quantity'] * df['price']

# Introduce missing values for demonstration
df.loc[np.random.choice(df.index, 10), 'price'] = np.nan
df.loc[np.random.choice(df.index, 5), 'region'] = np.nan

print(f"Dataset shape: {df.shape}")
print("\nMissing values:\n", df.isnull().sum())

# Step 2: Handle missing values
print("\n2. Handling missing values...")
df['price'] = df['price'].fillna(df['price'].mean())  # Fill price with mean
df['region'] = df['region'].fillna(df['region'].mode()[0])  # Fill region with mode
print("Missing values after handling:\n", df.isnull().sum())

# Step 3: Encode categorical variables
print("\n3. Encoding categorical variables...")
df = pd.get_dummies(df, columns=['product', 'region', 'salesperson'])
print("After one-hot encoding:\n", df.head())

# Step 4: Scale numerical features
print("\n4. Scaling numerical features...")
scaler = StandardScaler()
df[['quantity', 'price', 'total_sales']] = scaler.fit_transform(df[['quantity', 'price', 'total_sales']])
print("After standardization:\n", df[['quantity', 'price', 'total_sales']].head())

# Step 5: Save cleaned dataset
df.to_csv('cleaned_sales.csv', index=False)
print("\nCleaned dataset saved as 'cleaned_sales.csv'")
print("\n=== Task Complete! ===")

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

•Key Takeaways

Missing Data: Use dropna(), fillna() with mean/mode, or interpolation.
Encoding: Label encoding for ordinal, one-hot for nominal data.
Scaling: StandardScaler for z-scores, MinMaxScaler for fixed ranges.
Pandas Integration: Seamless with DataFrame operations.

Run the task code to clean and preprocess the dataset. Tomorrow, you'll explore SQL with Python!

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 6: Introduction to SQL with Python
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is SQL and SQLite?

SQL (Structured Query Language) is used to manage and query data in relational databases. SQLite is a lightweight, serverless database integrated into Python via the sqlite3 module, ideal for small-scale applications and learning.
Dependencies: sqlite3 (built into Python), pandas for data handling.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1. Connecting to SQLite

import sqlite3

# Connect to a database (creates file if it doesn't exist)
conn = sqlite3.connect('mydb.db')
cursor = conn.cursor()  # Cursor to execute SQL commands

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. Creating a Table

 Create a table
 cursor.execute('''
    CREATE TABLE IF NOT EXISTS employees (
        id INTEGER PRIMARY KEY,
        name TEXT,
        age INTEGER,
        department TEXT
    )
''')
conn.commit()  # Save changes

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. Basic SQL Queries

INSERT: Add data
# Insert one row
cursor.execute("INSERT INTO employees (name, age, department) VALUES (?, ?, ?)", 
               ('Alice', 25, 'IT'))

# Insert multiple rows
employees = [('Bob', 30, 'HR'), ('Charlie', 35, 'Finance')]
cursor.executemany("INSERT INTO employees (name, age, department) VALUES (?, ?, ?)", 
                   employees)
conn.commit()

SELECT: Retrieve data
# Select all
cursor.execute("SELECT * FROM employees")
rows = cursor.fetchall()
for row in rows:
    print(row)  # (1, 'Alice', 25, 'IT'), (2, 'Bob', 30, 'HR'), ...

# Select with condition
cursor.execute("SELECT name, age FROM employees WHERE age > 25")
print(cursor.fetchall())  # [('Bob', 30), ('Charlie', 35)]
JOIN: Combine tables

# Create another table
cursor.execute('''
    CREATE TABLE IF NOT EXISTS departments (
        dept_id INTEGER PRIMARY KEY,
        dept_name TEXT
    )
''')
cursor.executemany("INSERT INTO departments (dept_name) VALUES (?)", 
                   [('IT',), ('HR',), ('Finance',)])
conn.commit()

# Join tables
cursor.execute('''
    SELECT employees.name, employees.department, departments.dept_name
    FROM employees
    JOIN departments ON employees.department = departments.dept_name
''')
print(cursor.fetchall())  # [('Alice', 'IT', 'IT'), ('Bob', 'HR', 'HR'), ...]

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

4. Closing Connection

conn.close()  # Always close the connection

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Day 6 Task: SQLite Database Operations

Create a SQLite database, insert sales data from Day 2, and query it.

sqlite_sales.py

import sqlite3
import pandas as pd
import numpy as np

print("=== SQLite Database Task ===")

# Step 1: Create and load sales data
np.random.seed(42)
n_rows = 10
sales_data = {
    'product': np.random.choice(['Laptop', 'Phone', 'Tablet'], n_rows),
    'quantity': np.random.randint(1, 20, n_rows),
    'price': np.random.uniform(100, 1000, n_rows)
}
df = pd.DataFrame(sales_data)
df['total_sales'] = df['quantity'] * df['price']
print("\n1. Sample sales data:\n", df.head())

# Step 2: Create SQLite database and table
print("\n2. Creating SQLite database...")
conn = sqlite3.connect('sales.db')
cursor = conn.cursor()
cursor.execute('''
    CREATE TABLE IF NOT EXISTS sales (
        id INTEGER PRIMARY KEY,
        product TEXT,
        quantity INTEGER,
        price REAL,
        total_sales REAL
    )
''')
conn.commit()

# Step 3: Insert data
print("\n3. Inserting data...")
for _, row in df.iterrows():
    cursor.execute("INSERT INTO sales (product, quantity, price, total_sales) VALUES (?, ?, ?, ?)",
                  (row['product'], row['quantity'], row['price'], row['total_sales']))
conn.commit()

# Step 4: Query data
print("\n4. Querying data...")
# Select all
cursor.execute("SELECT * FROM sales LIMIT 5")
print("First 5 rows:", cursor.fetchall())

# Average total sales by product
cursor.execute("SELECT product, AVG(total_sales) FROM sales GROUP BY product")
print("Average sales by product:", cursor.fetchall())

# Step 5: Close connection
conn.close()
print("\nTask Complete! Database 'sales.db' created and queried.")

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

•Key Takeaways

SQLite: Lightweight, no server needed, uses sqlite3 module.
Connection: Use connect() and cursor() for operations.
Queries: INSERT adds data, SELECT retrieves, JOIN combines tables.
Commit: Save changes with conn.commit().
Close: Always close connections with conn.close().

Run the task code to create and query the database. Tomorrow, you'll build a mini project combining these skills!

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 7: Mini Project: Data Analysis Pipeline
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Goal: Build a simple end-to-end pipeline using NumPy, Pandas, Matplotlib/Seaborn, and SQLite.

Tools Used

pandas → load, clean, analyze
numpy → numerical operations
matplotlib / seaborn → visualize
sqlite3 → store & query data

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Pipeline Steps (Minimal Example)

# pipeline.py
import pandas as pd
import numpy as np
import sqlite3
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Generate sample sales data
np.random.seed(42)
df = pd.DataFrame({
    'date': pd.date_range('2023-01-01', periods=50),
    'product': np.random.choice(['A', 'B', 'C'], 50),
    'sales': np.random.randint(100, 1000, 50),
    'region': np.random.choice(['North', 'South'], 50)
})

# 2. Clean: Add missing values & fix
df.loc[::10, 'sales'] = np.nan
df['sales'] = df['sales'].fillna(df['sales'].mean())

# 3. Save to SQLite
conn = sqlite3.connect('pipeline.db')
df.to_sql('sales', conn, if_exists='replace', index=False)

# 4. Query from DB
query = "SELECT product, AVG(sales) as avg_sales FROM sales GROUP BY product"
result = pd.read_sql(query, conn)
print("Average sales by product:\n", result)

# 5. Visualize
sns.set_style('whitegrid')
plt.figure(figsize=(6,4))
sns.barplot(x='product', y='avg_sales', data=result)
plt.title('Avg Sales by Product')
plt.savefig('avg_sales_plot.png')
plt.close()

conn.close()
print("Pipeline complete! DB + Plot saved.")

Output

pipeline.db → SQLite database
avg_sales_plot.png → Bar chart

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Takeaways

Step,         Tool,    Purpose
Load & Clean, Pandas,  Handle missing data
Store,        SQLite,  Persistent storage
Query,        SQL,     Extract insights
Visualize,    Seaborn, Communicate results

Run it → One file, full pipeline.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 8: Web Scraping with BeautifulSoup
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is Web Scraping?

Extracting data from websites using Python.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Tools:

requests → get webpage
BeautifulSoup → parse HTML

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Install:
python -m pip install requests beautifulsoup4

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Minimal Example: Scrape Quotes

# scrape_quotes.py
import requests
from bs4 import BeautifulSoup
import csv

# 1. Get webpage
url = "http://quotes.toscrape.com"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# 2. Find all quotes
quotes = soup.find_all('div', class_='quote')

# 3. Extract text and author
data = []
for q in quotes:
    text = q.find('span', class_='text').get_text()
    author = q.find('small', class_='author').get_text()
    data.append([text, author])

# 4. Save to CSV
with open('quotes.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(['Quote', 'Author'])
    writer.writerows(data)

print("Scraped and saved to quotes.csv")

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Points

Step,       Code,                   Purpose
Get HTML,   requests.get(),         Download page
Parse,      BeautifulSoup(),        Turn HTML into navigable object
Find,       ".find(), .find_all()", Locate tags
Extract,    .get_text(),            Get content
Save,       csv.writer,             Export data

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Output: quotes.csv with quote and author.
Note: Always check robots.txt and respect website terms.

Done!
Tomorrow: APIs with requests1.8s

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 9: APIs and requests Library
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is an API?

API = Application Programming Interface

REST API = Web service that returns data (usually JSON) via HTTP.

Tools:

requests → make HTTP requests
json → parse JSON data

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Minimal Example: GitHub Public API (No Key Needed)

# api_github.py
import requests

# 1. GET request to public API
url = "https://api.github.com/users/octocat"
response = requests.get(url)

# 2. Check status
if response.status_code == 200:
    data = response.json()  # Parse JSON
    print("Name:", data['name'])
    print("Public Repos:", data['public_repos'])
    print("Followers:", data['followers'])
else:
    print("Error:", response.status_code)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

POST Request (Example with JSONPlaceholder)

# api_post.py
url = "https://jsonplaceholder.typicode.com/posts"
payload = {"title": "Test", "body": "Hello", "userId": 1}

response = requests.post(url, json=payload)
print("Created Post ID:", response.json()['id'])

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Points

Method,     Code,                                Use
GET,        requests.get(url),                   Fetch data
POST,       "requests.post(url, json=data)",     Send data
JSON,       response.json(),                     Convert response to Python dict

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Output:
textName: The Octocat
Public Repos: 8
Followers: 10000

Done!
Tomorrow: Flask Web App

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 10: Web Development with Flask
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is Flask?

Flask = Lightweight Python web framework.
Build simple web apps fast.

Install:
pip install flask

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Project Structure

myapp/
├── app.py
└── templates/
    ├── index.html
    └── form.html

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Minimal Flask App

1. app.py

from flask import Flask, render_template, request

app = Flask(__name__)

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/submit', methods=['GET', 'POST'])
def submit():
    if request.method == 'POST':
        name = request.form['name']
        return f"Hello, {name}!"
    return render_template('form.html')

if __name__ == '__main__':
    app.run(debug=True)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. templates/index.html

<!DOCTYPE html>
<h1>Welcome!</h1>
<a href="/submit">Go to Form</a>

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. templates/form.html

<form method="POST">
    Name: <input type="text" name="name">
    <input type="submit" value="Submit">
</form>

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Run It

python app.py

Visit: http://localhost:5000

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Points

Part,               Purpose
@app.route('/'),    Define URL
render_template(),  Show HTML
request.form,       Get form data
methods=['POST'],   Allow form submit

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Done!
You have a working Flask app with home + form.
Tomorrow: Automation with Selenium or PyAutoGUI

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 11: Automation with pyautogui or selenium
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is Automation?

Use code to control browser (Selenium) or mouse/keyboard (PyAutoGUI) to repeat tasks.

Focus: Selenium (browser automation)

Install:
pip install selenium

Need ChromeDriver (auto-managed with webdriver-manager)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


Minimal Example: Fill a Form

# auto_form.py
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# 1. Open browser
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))

# 2. Go to form
driver.get("https://httpbin.org/forms/post")

# 3. Fill form
driver.find_element(By.NAME, "custname").send_keys("Alice")
driver.find_element(By.NAME, "custtel").send_keys("123456789")
driver.find_element(By.NAME, "custemail").send_keys("a@example.com")
driver.find_element(By.NAME, "size").click()  # Select pizza size
driver.find_element(By.XPATH, "//input[@value='bacon']").click()  # Topping

# 4. Submit
driver.find_element(By.XPATH, "//button[text()='Submit Order']").click()

# 5. Wait & close
import time
time.sleep(3)
driver.quit()

print("Form submitted!")

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Points

Action,                 Code
Find element,           "By.NAME, By.XPATH"
Type,                   ".send_keys(""text"")"
Click,                  .click()
Wait,                   time.sleep(2)
Close,                  .quit()

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Output: Opens Chrome, fills form, submits.

Done!
Tomorrow: Async Programming with asyncio

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 12: Asynchronous Programming with asyncio
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is Async?

Run multiple I/O tasks (like API calls) concurrently without blocking.

Great for waiting (network, files), not CPU-heavy work.

Tools:

asyncio → event loop
aiohttp → async HTTP requests

Install:
bashpip install aiohttp

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Minimal Example: Fetch 3 APIs Concurrently

# async_apis.py
import asyncio
import aiohttp

async def fetch(session, url):
    async with session.get(url) as resp:
        return await resp.json()

async def main():
    urls = [
        "https://api.github.com/users/octocat",
        "https://httpbin.org/delay/1",
        "https://jsonplaceholder.typicode.com/posts/1"
    ]
    
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        
        for r in results:
            print("Data:", list(r.keys())[:2])  # Show first 2 keys

# Run
asyncio.run(main())

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Points

Term,             Meaning
async def,        Coroutine function
await,            Pause until done
asyncio.gather(), Run many tasks together
aiohttp,          Async version of requests

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Output (in ~1 second):
textData: ['login', 'id']
Data: ['url', 'args']
Data: ['userId', 'id']

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Done!
You fetched 3 APIs at the same time.
Tomorrow: Machine Learning with Scikit-learn

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 13: Introduction to Machine Learning with Scikit-learn
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is ML?

Machine Learning = Make computer predict or classify using data.

Types:

Regression → Predict number (e.g., house price)
Classification → Predict category (e.g., spam/not spam)


Tools
pip install scikit-learn pandas

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Minimal Example: Predict House Prices

# ml_house.py
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# 1. Sample data (size, price)
data = {
    'size': [500, 750, 1000, 1250, 1500, 1750, 2000],
    'price': [150000, 200000, 250000, 300000, 350000, 400000, 450000]
}
df = pd.DataFrame(data)

# 2. Features (X) and target (y)
X = df[['size']]      # Input
y = df['price']       # Output

# 3. Split: 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Model
model = LinearRegression()
model.fit(X_train, y_train)

# 5. Predict
pred = model.predict([[1200]])  # Predict price for 1200 sqft
print(f"Predicted price for 1200 sqft: ${pred[0]:,.0f}")

Output
textPredicted price for 1200 sqft: $299,167

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Points

Step,    Code,                 Purpose
Data,    pandas,               Load & prepare
Split,   train_test_split,     Avoid overfitting
Model,   LinearRegression(),   Learn pattern
Train,   ".fit(X, y)",         Learn from data
Predict, .predict([[x]]),      Use model

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Done!
You built a working ML model.
Tomorrow: NoSQL with MongoDB3.8s

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 14: Working with Databases (NoSQL with MongoDB)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

What is MongoDB?


NoSQL database that stores data in JSON-like documents (flexible, no fixed schema).

Tool: pymongo → Python driver

Install & Setup

bashpip install pymongo

Start MongoDB locally (or use MongoDB Atlas):
mongod

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Minimal Example: CRUD with pymongo

# mongo_crud.py
from pymongo import MongoClient

# 1. Connect
client = MongoClient('mongodb://localhost:27017/')
db = client['mydb']
collection = db['users']

# 2. INSERT (Create)
user = {"name": "Alice", "age": 25, "city": "NYC"}
collection.insert_one(user)
print("Inserted Alice")

# 3. READ (Find)
for u in collection.find({"age": {"$gt": 20}}):
    print(u)

# 4. UPDATE
collection.update_one({"name": "Alice"}, {"$set": {"age": 26}})
print("Updated Alice's age")

# 5. DELETE
collection.delete_one({"name": "Alice"})
print("Deleted Alice")

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Points

Operation,    Code
Connect,      MongoClient()
Insert,      .insert_one(doc)
Find,        .find(filter)
Update,      ".update_one(filter, update)"
Delete,     .delete_one(filter)

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Output:

Inserted Alice
{'name': 'Alice', 'age': 25, 'city': 'NYC', ...}
Updated Alice's age
Deleted Alice

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Done!
You used MongoDB with Python.
Tomorrow: Capstone Project

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Day 15: Capstone Project and Review
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Goal
Build a simple end-to-end app using everything learned.

Project: GitHub User Analyzer

Features:

Fetch user data via GitHub API
Store in MongoDB
Analyze with Pandas
Display with Flask + Chart


Project Structure

capstone/
├── app.py          # Flask app
├── fetch.py        # API + MongoDB
├── analyze.py      # Pandas analysis
└── templates/
    └── result.html

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

1. fetch.py – API + MongoDB

# fetch.py
import requests
from pymongo import MongoClient

username = "octocat"
url = f"https://api.github.com/users/{username}"
data = requests.get(url).json()

client = MongoClient('mongodb://localhost:27017/')
db = client['github']
db.users.insert_one(data)
print("Saved to MongoDB")

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

2. analyze.py – Pandas

# analyze.py
import pandas as pd
from pymongo import MongoClient

client = MongoClient('mongodb://localhost:27017/')
data = list(client.github.users.find())
df = pd.DataFrame(data)

print(f"Followers: {df['followers'].iloc[0]}")
print(f"Repos: {df['public_repos'].iloc[0]}")

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

3. app.py – Flask

# app.py
from flask import Flask, render_template
from pymongo import MongoClient

app = Flask(__name__)

@app.route('/')
def home():
    client = MongoClient('mongodb://localhost:27017/')
    user = client.github.users.find_one()
    return render_template('result.html', user=user)

if __name__ == '__main__':
    app.run(debug=True)

4. templates/result.html
html<h1>{{ user.name }}</h1>
<p>Followers: {{ user.followers }}</p>
<p>Public Repos: {{ user.public_repos }}</p>

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Run Order

python fetch.py
python analyze.py
python app.py

Visit: http://localhost:5000

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Key Skills Used

Skill,    Used In
API,      requests
NoSQL,    pymongo
Analysis, pandas
Web,      Flask

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Done!
You built a portfolio-ready project.
Push to GitHub → Share link → Done!
Congratulations! 15-Day Python Journey Complete

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                                      END - THANK YOU
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
